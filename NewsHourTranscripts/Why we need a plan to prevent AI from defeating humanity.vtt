WEBVTT

00:02.033 --> 00:05.766 align:left position:0%,start line:80% size:100%
JUDY WOODRUFF: Now: the
fears around the development
of artificial intelligence.

00:05.766 --> 00:10.766 align:left position:0%,start line:80% size:100%
Computer superintelligence is
a long, long way from the stuff
of sci-fi movies, but several

00:12.733 --> 00:16.166 align:left position:12.5%,start line:73.33% size:87.5%
high-profile leaders
and thinkers have been
worrying quite publicly
about what they see

00:16.900 --> 00:18.533 align:left position:12.5%,start line:93.33% size:87.5%
as the risks to come.

00:18.533 --> 00:21.700 align:left position:0%,start line:86.67% size:100%
Our economics correspondent,
Paul Solman, explores that.

00:21.700 --> 00:24.100 align:left position:12.5%,start line:86.67% size:87.5%
It's part of his weekly
series, Making Sense.

00:24.100 --> 00:29.100 align:left position:0%,start line:80% size:100%
ACTOR: I want to talk to you
about the greatest scientific
event in the history of man.

00:30.533 --> 00:32.800 align:left position:0%,start line:93.33% size:100%
ACTOR: Are you building an A.I.?

00:32.800 --> 00:34.666 align:left position:12.5%,start line:86.67% size:87.5%
PAUL SOLMAN: A.I.,
artificial intelligence.

00:34.666 --> 00:37.000 align:left position:12.5%,start line:86.67% size:87.5%
ACTRESS: Do you think I
might be switched off?

00:37.000 --> 00:38.366 align:left position:0%,start line:93.33% size:100%
ACTOR: It's not up to me.

00:38.366 --> 00:40.433 align:left position:0%,start line:93.33% size:100%
ACTRESS: Why is it up to anyone?

00:40.433 --> 00:44.400 align:left position:0%,start line:80% size:100%
PAUL SOLMAN: Some version of
this scenario has had prominent
tech luminaries and scientists

00:44.400 --> 00:46.400 align:left position:12.5%,start line:93.33% size:87.5%
worried for years.

00:46.400 --> 00:50.033 align:left position:0%,start line:86.67% size:100%
In 2014, cosmologist Stephen
Hawking told the BBC:

00:50.033 --> 00:53.333 align:left position:0%,start line:73.33% size:100%
STEPHEN HAWKING, Scientist
(through computer voice):
I think the development
of full artificial

00:53.333 --> 00:56.966 align:left position:0%,start line:86.67% size:100%
intelligence could spell
the end of the human race.

00:56.966 --> 01:01.966 align:left position:0%,start line:73.33% size:100%
PAUL SOLMAN: And just this
week, Tesla and SpaceX
entrepreneur Elon Musk
told the ÑMDNMÑNational

01:02.866 --> 01:05.266 align:left position:12.5%,start line:93.33% size:87.5%
Governors Association:

01:05.266 --> 01:08.566 align:left position:0%,start line:73.33% size:100%
ELON MUSK, CEO, Tesla
Motors: A.I. is a
fundamental existential
risk for human civilization.

01:08.566 --> 01:11.666 align:left position:12.5%,start line:86.67% size:87.5%
And I don't think people
fully appreciate that.

01:11.666 --> 01:14.466 align:left position:0%,start line:86.67% size:100%
PAUL SOLMAN: OK, but
what's the economics angle?

01:14.466 --> 01:19.466 align:left position:0%,start line:80% size:100%
Well, at Oxford University's
Future of Humanity Institute,
founding director Nick Bostrom

01:21.466 --> 01:25.300 align:left position:0%,start line:80% size:100%
leads a team trying to figure
out how best to invest in,
well, the future of humanity.

01:27.200 --> 01:29.733 align:left position:0%,start line:80% size:100%
NICK BOSTROM, Oxford University:
We are in this very peculiar
situation of looking back

01:29.733 --> 01:34.733 align:left position:0%,start line:80% size:100%
at the history of our species,
100,000 years old, and now
finding ourselves just before

01:36.633 --> 01:39.866 align:left position:0%,start line:80% size:100%
the threshold to what looks
like it will be this transition
to some post-human era of

01:39.866 --> 01:43.500 align:left position:0%,start line:73.33% size:100%
superintelligence that
can colonize the universe,
and then maybe last
for billions of years.

01:43.500 --> 01:47.633 align:left position:0%,start line:80% size:100%
PAUL SOLMAN: Philosopher Bostrom
has been perhaps the most
prominent thinker about the

01:47.633 --> 01:52.633 align:left position:12.5%,start line:73.33% size:87.5%
benefits and dangers
to humanity of what he
calls superintelligence
for many years.

01:54.633 --> 01:57.066 align:left position:0%,start line:80% size:100%
NICK BOSTROM: Once there is
superintelligence, the fate of
humanity may depend on what that

01:57.066 --> 01:59.600 align:left position:12.5%,start line:93.33% size:87.5%
superintelligence does.

01:59.600 --> 02:02.733 align:left position:0%,start line:73.33% size:100%
PAUL SOLMAN: There are
plenty of ways to invest in
humanity, he says, giving
money to anti-disease

02:02.733 --> 02:05.166 align:left position:12.5%,start line:93.33% size:87.5%
charities, for example.

02:05.166 --> 02:09.066 align:left position:0%,start line:73.33% size:100%
But Bostrom thinks
longer-term, about investing
to lessen existential
risks, those that threaten

02:11.333 --> 02:14.133 align:left position:12.5%,start line:86.67% size:87.5%
to wipe out the human
species entirely.

02:14.133 --> 02:15.933 align:left position:0%,start line:93.33% size:100%
Global warming might be one.

02:15.933 --> 02:18.933 align:left position:0%,start line:86.67% size:100%
But plenty of other people are
worrying about that, he says.

02:18.933 --> 02:21.566 align:left position:0%,start line:93.33% size:100%
So, he thinks about other risks.

02:21.566 --> 02:24.033 align:left position:12.5%,start line:86.67% size:87.5%
What are the greatest
of those risks?

02:24.033 --> 02:28.133 align:left position:0%,start line:73.33% size:100%
NICK BOSTROM: The
greatest existential
risks arise from certain
anticipated technological

02:30.566 --> 02:33.566 align:left position:0%,start line:73.33% size:100%
breakthroughs that we
might make, in particular,
machine superintelligence,
nanotechnology,

02:38.500 --> 02:43.500 align:left position:12.5%,start line:73.33% size:87.5%
and synthetic biology,
fundamentally because we
don't have the ability
to uninvent anything

02:45.633 --> 02:47.733 align:left position:25%,start line:93.33% size:75%
that we invent.

02:47.733 --> 02:51.500 align:left position:12.5%,start line:73.33% size:87.5%
We don't, as a human
civilization, have the
ability to put the genie
back into the bottle.

02:51.500 --> 02:55.233 align:left position:0%,start line:80% size:100%
Once something has been
published, then we are
stuck with that knowledge.

02:55.233 --> 02:59.233 align:left position:12.5%,start line:80% size:87.5%
PAUL SOLMAN: So Bostrom
wants money invested
in how to manage A.I.

02:59.233 --> 03:03.633 align:left position:0%,start line:80% size:100%
NICK BOSTROM: Specifically
on the question, if and when
in the future you could build

03:03.633 --> 03:08.633 align:left position:0%,start line:73.33% size:100%
machines that were really
smart, maybe superintelligent,
smarter than humans, how
could you then ensure

03:10.566 --> 03:13.266 align:left position:0%,start line:80% size:100%
that you could control what
those machines do, that they
were beneficial, that they were

03:13.266 --> 03:15.366 align:left position:0%,start line:93.33% size:100%
aligned with human intentions?

03:15.366 --> 03:20.166 align:left position:0%,start line:80% size:100%
PAUL SOLMAN: How likely is it
that machines would develop
basically a mind of their own,

03:21.266 --> 03:23.233 align:left position:12.5%,start line:86.67% size:87.5%
which is what you're
saying, right?

03:23.233 --> 03:26.900 align:left position:0%,start line:80% size:100%
NICK BOSTROM: I do think
that advanced A.I., including
superintelligence, is a sort of

03:29.300 --> 03:33.433 align:left position:0%,start line:73.33% size:100%
portal through which
humanity will have passage,
assuming we don't destroy
ourselves prematurely

03:35.233 --> 03:36.533 align:left position:12.5%,start line:93.33% size:87.5%
in some other way.

03:36.533 --> 03:40.600 align:left position:12.5%,start line:86.67% size:87.5%
Right now, the human
brain is where it's at.

03:40.600 --> 03:44.566 align:left position:0%,start line:86.67% size:100%
It's the source of almost all
of the technologies we have.

03:44.566 --> 03:46.133 align:left position:12.5%,start line:86.67% size:87.5%
PAUL SOLMAN: I'm
relieved to hear that.

03:46.133 --> 03:47.633 align:left position:25%,start line:93.33% size:75%
(LAUGHTER)

03:47.633 --> 03:49.133 align:left position:0%,start line:86.67% size:100%
NICK BOSTROM: And the complex
social organization we have.

03:49.133 --> 03:51.266 align:left position:12.5%,start line:93.33% size:87.5%
PAUL SOLMAN: Right.

03:51.266 --> 03:52.966 align:left position:12.5%,start line:73.33% size:87.5%
NICK BOSTROM: It's why
the modern condition is
so different from the
way that the chimpanzees

03:52.966 --> 03:55.000 align:left position:37.5%,start line:93.33% size:62.5%
live.

03:55.000 --> 03:58.966 align:left position:0%,start line:80% size:100%
It's all through the
human brain's ability to
discover and communicate.

04:01.033 --> 04:04.166 align:left position:0%,start line:80% size:100%
But there is no reason to
think that human intelligence
is anywhere near the greatest

04:06.233 --> 04:10.633 align:left position:0%,start line:80% size:100%
possible level of intelligence
that could exist, that we are
sort of the smartest possible

04:11.100 --> 04:13.166 align:left position:37.5%,start line:93.33% size:62.5%
species.

04:13.166 --> 04:16.733 align:left position:0%,start line:80% size:100%
I think, rather, that we are
the stupidest possible species
that is capable of creating

04:17.600 --> 04:20.133 align:left position:0%,start line:93.33% size:100%
technological civilization.

04:20.133 --> 04:23.600 align:left position:0%,start line:73.33% size:100%
PAUL SOLMAN: And capable
of creating technology
that has begun to surpass
us, first in chess,

04:31.966 --> 04:36.966 align:left position:0%,start line:80% size:100%
then in "Jeopardy," now in
the supposedly impossible
game for a machine to win, Go.

04:49.033 --> 04:54.033 align:left position:0%,start line:80% size:100%
This is just task-oriented
software, some have argued,
and not really intelligence at

04:54.500 --> 04:56.533 align:left position:37.5%,start line:93.33% size:62.5%
all.

04:56.533 --> 04:58.966 align:left position:0%,start line:80% size:100%
Moreover, whatever you call
it, there will be enormous
benefits, says Bostrom.

04:58.966 --> 05:03.966 align:left position:0%,start line:80% size:100%
On the other hand, if we
approach real intelligence,
it could also become a threat.

05:06.033 --> 05:09.233 align:left position:0%,start line:80% size:100%
Think of "Ex Machina" or "The
Matrix" or Elon Musk's fantasy
fear this week about advanced

05:27.066 --> 05:29.100 align:left position:37.5%,start line:93.33% size:62.5%
A.I.

05:29.100 --> 05:32.166 align:left position:0%,start line:80% size:100%
ELON MUSK: Well, it could start
a war by create - - by doing
fake news and spoofing e-mail

05:32.166 --> 05:37.166 align:left position:0%,start line:80% size:100%
accounts and fake press
releases, and just by, you
know, manipulating information.

05:39.566 --> 05:41.566 align:left position:12.5%,start line:86.67% size:87.5%
The pen is mightier
than the sword.

05:41.566 --> 05:45.500 align:left position:0%,start line:80% size:100%
PAUL SOLMAN: So, this is going
to be a cat-and-mouse game
between us and the intelligence?

05:45.500 --> 05:47.733 align:left position:12.5%,start line:86.67% size:87.5%
NICK BOSTROM: That
would be one model.

05:47.733 --> 05:52.733 align:left position:0%,start line:80% size:100%
One line of attack is to try to
leverage the A.I.'s intelligence
to learn what it is that

05:54.566 --> 05:56.833 align:left position:12.5%,start line:86.67% size:87.5%
we value and what
we want it to do.

05:56.833 --> 06:01.800 align:left position:0%,start line:80% size:100%
PAUL SOLMAN: In order to protect
ourselves from what could
be a truly existential risk.

06:01.800 --> 06:06.433 align:left position:0%,start line:80% size:100%
So, how do you get the greatest
good for the greatest number
of present and future humans

06:06.433 --> 06:08.533 align:left position:37.5%,start line:93.33% size:62.5%
beings?

06:08.533 --> 06:12.233 align:left position:0%,start line:80% size:100%
It might be to invest now
in controlling the evolution
of artificial intelligence.

06:12.233 --> 06:16.833 align:left position:0%,start line:80% size:100%
For the "PBS NewsHour," this
is economics correspondent
Paul Solman, reporting from

06:16.833 --> 06:21.833 align:left position:25%,start line:93.33% size:75%
Oxford, England.
